{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Rai AI ‚Äì Dataset Builder (Rice & Durian) üì∏üåæ\n",
        "\n",
        "This notebook downloads ~90 images per class for **9 tags** (rice & durian), cleans duplicates/blurry images, and **zips one file per tag** for **Azure Custom Vision**.\n",
        "\n",
        "### How to use (Google Colab recommended)\n",
        "1. Run each cell from top to bottom.\n",
        "2. Edit `TAGS` if needed. Keep labels **exactly** as in your app.\n",
        "3. After the last step, download the **ZIP files** from the `/content/datasets_zips` folder.\n",
        "\n",
        "> Legal note: default search uses the **bing-image-downloader** which fetches publicly available images. Use for research/MVP; if you commercialize, prefer datasets with clear licenses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip -q install bing-image-downloader pillow imagehash opencv-python-headless tqdm\n",
        "print(\"‚úÖ Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import shutil, os, re\n",
        "from PIL import Image\n",
        "import imagehash\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "OUT_ROOT = Path(\"/content/datasets\")\n",
        "ZIPS_ROOT = Path(\"/content/datasets_zips\")\n",
        "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "ZIPS_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "TAGS = [\n",
        "    (\"rice_brown_spot\",     \"rice brown spot disease leaf\"),\n",
        "    (\"rice_leaf_blast\",     \"rice leaf blast disease leaf\"),\n",
        "    (\"rice_n_deficiency\",   \"rice nitrogen deficiency leaf\"),\n",
        "    (\"rice_potassium_def\",  \"rice potassium deficiency leaf\"),\n",
        "    (\"rice_healthy\",        \"healthy rice leaf\"),\n",
        "    (\"durian_anthracnose\",  \"durian anthracnose leaf\"),\n",
        "    (\"durian_n_deficiency\", \"durian nitrogen deficiency leaf\"),\n",
        "    (\"durian_mealybug\",     \"durian mealybug leaf\"),\n",
        "    (\"durian_healthy\",      \"healthy durian leaf\"),\n",
        "]\n",
        "\n",
        "TARGET_PER_TAG = 120\n",
        "MIN_W, MIN_H = 224, 224\n",
        "BLUR_THRESHOLD = 50.0\n",
        "HAMMING_NEAR_DUP = 4\n",
        "print(\"Config ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bing_image_downloader import downloader\n",
        "\n",
        "for label, query in TAGS:\n",
        "    out_dir = OUT_ROOT / label\n",
        "    out_dir.mkdir(exist_ok=True, parents=True)\n",
        "    print(f\"\\nüîé Downloading: {label} ‚Üê '{query}'\")\n",
        "    downloader.download(query,\n",
        "                        limit=TARGET_PER_TAG,\n",
        "                        output_dir=str(OUT_ROOT),\n",
        "                        adult_filter_off=True,\n",
        "                        force_replace=False,\n",
        "                        timeout=60)\n",
        "    # move from query folder into label folder\n",
        "    qdir = OUT_ROOT / query\n",
        "    if qdir.exists():\n",
        "        for p in qdir.rglob('*'):\n",
        "            if p.is_file():\n",
        "                dest = out_dir / p.name\n",
        "                try:\n",
        "                    shutil.move(str(p), str(dest))\n",
        "                except Exception:\n",
        "                    pass\n",
        "        shutil.rmtree(qdir, ignore_errors=True)\n",
        "print(\"\\n‚úÖ Download step finished\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def is_blurry(p):\n",
        "    img = cv2.imdecode(np.fromfile(p, dtype=np.uint8), cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None:\n",
        "        return True\n",
        "    fm = cv2.Laplacian(img, cv2.CV_64F).var()\n",
        "    return fm < BLUR_THRESHOLD\n",
        "\n",
        "def clean_dir(tag_dir: Path):\n",
        "    # Remove non-images and too small\n",
        "    for p in list(tag_dir.iterdir()):\n",
        "        if not p.is_file():\n",
        "            continue\n",
        "        try:\n",
        "            with Image.open(p) as im:\n",
        "                w, h = im.size\n",
        "                if w < MIN_W or h < MIN_H:\n",
        "                    p.unlink(missing_ok=True); continue\n",
        "        except Exception:\n",
        "            p.unlink(missing_ok=True); continue\n",
        "\n",
        "    # Remove blurry\n",
        "    for p in list(tag_dir.iterdir()):\n",
        "        if not p.is_file():\n",
        "            continue\n",
        "        try:\n",
        "            if is_blurry(str(p)):\n",
        "                p.unlink(missing_ok=True)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Deduplicate by perceptual hash\n",
        "    hashes = {}\n",
        "    for p in list(tag_dir.iterdir()):\n",
        "        if not p.is_file():\n",
        "            continue\n",
        "        try:\n",
        "            with Image.open(p) as im:\n",
        "                h = imagehash.phash(im)\n",
        "        except Exception:\n",
        "            p.unlink(missing_ok=True); continue\n",
        "        similar = next((h0 for h0 in hashes.keys() if h - h0 <= HAMMING_NEAR_DUP), None)\n",
        "        if similar is not None:\n",
        "            p.unlink(missing_ok=True)\n",
        "        else:\n",
        "            hashes[h] = p\n",
        "\n",
        "for label, _ in TAGS:\n",
        "    print(f\"üßπ Cleaning {label}\")\n",
        "    clean_dir(OUT_ROOT / label)\n",
        "print(\"\\n‚úÖ Cleaning complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "MAX_KEEP = 90\n",
        "for label, _ in TAGS:\n",
        "    d = OUT_ROOT / label\n",
        "    files = [p for p in d.iterdir() if p.is_file()]\n",
        "    files.sort()\n",
        "    if len(files) > MAX_KEEP:\n",
        "        for p in files[MAX_KEEP:]:\n",
        "            p.unlink(missing_ok=True)\n",
        "print(\"‚úÖ Downselected to ‚â§90 per tag\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "for label, _ in TAGS:\n",
        "    src = OUT_ROOT / label\n",
        "    zip_base = (Path('/content/datasets_zips') / label)\n",
        "    try:\n",
        "        shutil.make_archive(str(zip_base), 'zip', root_dir=src)\n",
        "        print('üì¶', str(zip_base) + '.zip')\n",
        "    except Exception as e:\n",
        "        print('Zip error', label, e)\n",
        "print(\"\\n‚úÖ All done. Download zips from /content/datasets_zips\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}